{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c-setup",
   "metadata": {},
   "source": [
    "# MPI Programming in C\n",
    "\n",
    "This notebook is a introduction to MPI programming in C. It covers the following topics:\n",
    "\n",
    "1. Basic MPI Environment: Initialization, Rank, Size, Finalization\n",
    "2. Blocking Communication: Blocking Send/Recv\n",
    "3. Non-blocking Communication: MPI_Isend, MPI_Irecv, MPI_Wait, MPI_Waitall\n",
    "4. Collective Communication: Broadcast, Scatter, Gather, Reduce, Allreduce, All-to-All\n",
    "5. Derived Datatypes: Creating custom MPI datatypes for complex structures\n",
    "6. Subcommunicators: Splitting MPI_COMM_WORLD into groups\n",
    "7. Topology Mapping: Cartesian grid communicators\n",
    "8. Performance Measurement: MPI_Wtime for timing\n",
    "\n",
    "Compile with `mpicc` and run with `mpiexec` or `mpirun`.\n",
    "\n",
    "For example:\n",
    "```bash\n",
    "mpiexec -n 4 ./mpi_example\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-basic",
   "metadata": {},
   "source": [
    "## 1. Basic MPI Environment\n",
    "\n",
    "Initialize the MPI environment, get each process's rank and the total number of processes, then finalize.\n",
    "\n",
    "`MPI_Init` : Initialize the MPI environment\n",
    "\n",
    "`MPI_Comm_rank` : Get the rank of the calling process in the communicator\n",
    "\n",
    "`MPI_Comm_size` : Get the total number of processes in the communicator\n",
    "\n",
    "`MPI_Finalize` : Finalize the MPI environment\n",
    "\n",
    "Code example\n",
    "\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "    int rank, size;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    printf(\"[Basic] Process %d of %d\\n\", rank, size);\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c-basic-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Basic] Process 0 of 4\n",
      "[Basic] Process 1 of 4\n",
      "[Basic] Process 2 of 4\n",
      "[Basic] Process 3 of 4\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Compile and run\n",
    "mpicc ./codes/1_hello_basic.c -o codes/1_hello_basic\n",
    "mpiexec -n 4 ./codes/1_hello_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-p2p",
   "metadata": {},
   "source": [
    "## 2. Blocking Communication\n",
    "\n",
    "Blocking communication using `MPI_Send` and `MPI_Recv`. Process 0 sends a message to process 1.\n",
    "\n",
    "\n",
    "\n",
    "`MPI_Send` : Send a message to another process\n",
    "\n",
    "`MPI_Recv` : Receive a message from another process\n",
    "\n",
    "Code example\n",
    "\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "  MPI_Init(&argc, &argv);\n",
    "  int rank, size;\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "  if (size < 2) {\n",
    "    if (rank == 0)\n",
    "      fprintf(stderr, \"Requires at least 2 processes\\n\");\n",
    "    MPI_Finalize();\n",
    "    return 1;\n",
    "  }\n",
    "  if (rank == 0) {\n",
    "    char msg[] = \"Hello from 0\";\n",
    "    MPI_Send(msg, sizeof(msg), MPI_CHAR, 1, 0, MPI_COMM_WORLD);\n",
    "    printf(\"[Blocking] Process 0 sent message.\\n\");\n",
    "  } else if (rank == 1) {\n",
    "    char msg[50];\n",
    "    MPI_Recv(msg, 50, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "    printf(\"[Blocking] Process 1 received: %s\\n\", msg);\n",
    "  }\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c-p2p-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Blocking] Process 0 sent message.\n",
      "[Blocking] Process 1 received: Hello from 0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Compile and run\n",
    "mpicc ./codes/2_blocking_comm.c -o ./codes/2_blocking_comm\n",
    "mpiexec -n 2 ./codes/2_blocking_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-nonblocking",
   "metadata": {},
   "source": [
    "## 3. Non-blocking Communication\n",
    "\n",
    "Using `MPI_Isend` and `MPI_Irecv` to allow computation and communication to overlap. Process 0 sends a message to process 1 and continues computation while process 1 receives the message in the background with a sleep to simulate computation.\n",
    "\n",
    "`MPI_Request` : Handle to a non-blocking operation\n",
    "\n",
    "`MPI_Wait` : used to wait for non-blocking operations to complete.\n",
    "\n",
    "`MPI_Isend` : Non-blocking send\n",
    "\n",
    "`MPI_Irecv` : Non-blocking receive\n",
    "\n",
    "Code example\n",
    "\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <unistd.h>\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "    int rank, size;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    if (size < 2) {\n",
    "        if (rank == 0) fprintf(stderr, \"Requires at least 2 processes\\n\");\n",
    "        MPI_Finalize();\n",
    "        return 1;\n",
    "    }\n",
    "    MPI_Request req;\n",
    "    if (rank == 0) {\n",
    "        int data = 123;\n",
    "        MPI_Isend(&data, 1, MPI_INT, 1, 1, MPI_COMM_WORLD, &req);\n",
    "        printf(\"[Nonblocking] Process 0 initiated Isend with data %d\\n\", data);\n",
    "        sleep(1);\n",
    "        MPI_Wait(&req, MPI_STATUS_IGNORE);\n",
    "        printf(\"[Nonblocking] Process 0 completed Isend\\n\");\n",
    "    } else if (rank == 1) {\n",
    "        int data = 0;\n",
    "        MPI_Irecv(&data, 1, MPI_INT, 0, 1, MPI_COMM_WORLD, &req);\n",
    "        printf(\"[Nonblocking] Process 1 initiated Irecv\\n\");\n",
    "        sleep(1);\n",
    "        MPI_Wait(&req, MPI_STATUS_IGNORE);\n",
    "        printf(\"[Nonblocking] Process 1 received data %d\\n\", data);\n",
    "    }\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c-nonblocking-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nonblocking] Process 1 initiated Irecv\n",
      "[Nonblocking] Process 0 initiated Isend with data 123\n",
      "[Nonblocking] Process 0 completed Isend\n",
      "[Nonblocking] Process 1 received data 123\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Compile and run\n",
    "mpicc ./codes/3_nonblocking.c -o ./codes/3_nonblocking\n",
    "mpiexec -n 2 ./codes/3_nonblocking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-collective",
   "metadata": {},
   "source": [
    "## 4. Collective Communication\n",
    "\n",
    "Demonstrate broadcast, scatter, gather, reduce, allreduce, and all-to-all collective operations.\n",
    "\n",
    "`MPI_Bcast` : Broadcast a message from one process to all others\n",
    "\n",
    "`MPI_Scatter` : Scatter an array from one process to all others\n",
    "\n",
    "`MPI_Gather` : Gather an array from all processes to one\n",
    "\n",
    "`MPI_Reduce` : Reduce an array from all processes to one\n",
    "\n",
    "`MPI_Allreduce` : Reduce an array from all processes to all\n",
    "\n",
    "`MPI_Alltoall` : Exchange data between all processes\n",
    "\n",
    "Code example\n",
    "\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <unistd.h>\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "  int rank, size;\n",
    "\n",
    "  MPI_Init(&argc, &argv);\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "\n",
    "  // MPI_Bcast: Broadcast a value from rank 0 to all processes.\n",
    "  int bcast_data;\n",
    "  if (rank == 0)\n",
    "    bcast_data = 100;\n",
    "  MPI_Bcast(&bcast_data, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "  printf(\"[Collective] Rank %d received bcast_data: %d\\n\", rank, bcast_data);\n",
    "  sleep(1);\n",
    "\n",
    "  // MPI_Scatter: Scatter an array of integers from rank 0.\n",
    "  int *scatter_sendbuf = NULL;\n",
    "  int scatter_recvbuf;\n",
    "  if (rank == 0) {\n",
    "    scatter_sendbuf = malloc(sizeof(int) * size);\n",
    "    for (int i = 0; i < size; i++) {\n",
    "      scatter_sendbuf[i] = (i + 1) * 10;\n",
    "    }\n",
    "  }\n",
    "  MPI_Scatter(scatter_sendbuf, 1, MPI_INT, &scatter_recvbuf, 1, MPI_INT, 0,\n",
    "              MPI_COMM_WORLD);\n",
    "  printf(\"[Collective] Rank %d received scatter_recvbuf: %d\\n\", rank,\n",
    "         scatter_recvbuf);\n",
    "  sleep(1);\n",
    "\n",
    "  // MPI_Gather: Gather a single integer from each process to rank 0.\n",
    "  int send_val = rank;\n",
    "  int *gather_recvbuf = NULL;\n",
    "  if (rank == 0) {\n",
    "    gather_recvbuf = malloc(sizeof(int) * size);\n",
    "  }\n",
    "  MPI_Gather(&send_val, 1, MPI_INT, gather_recvbuf, 1, MPI_INT, 0,\n",
    "             MPI_COMM_WORLD);\n",
    "  if (rank == 0) {\n",
    "    printf(\"[Collective] Rank 0 gathered values: \");\n",
    "    for (int i = 0; i < size; i++) {\n",
    "      printf(\"%d \", gather_recvbuf[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "  }\n",
    "  sleep(1);\n",
    "\n",
    "  // MPI_Reduce: Sum the values from each process, result goes to rank 0.\n",
    "  int reduce_sum;\n",
    "  MPI_Reduce(&send_val, &reduce_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n",
    "  if (rank == 0) {\n",
    "    printf(\"[Collective] Sum of ranks (reduce): %d\\n\", reduce_sum);\n",
    "  }\n",
    "  sleep(1);\n",
    "\n",
    "  // MPI_Allreduce: Sum the values from each process. All processes receive the\n",
    "  // result.\n",
    "  int allreduce_sum;\n",
    "  MPI_Allreduce(&send_val, &allreduce_sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n",
    "  printf(\"[Collective] Rank %d has allreduce sum: %d\\n\", rank, allreduce_sum);\n",
    "  sleep(1);\n",
    "\n",
    "  // MPI_Alltoall: Exchange data between all processes.\n",
    "  int *alltoall_send = malloc(sizeof(int) * size);\n",
    "  int *alltoall_recv = malloc(sizeof(int) * size);\n",
    "  for (int i = 0; i < size; i++) {\n",
    "    alltoall_send[i] = rank * 100 + i;\n",
    "  }\n",
    "  MPI_Alltoall(alltoall_send, 1, MPI_INT, alltoall_recv, 1, MPI_INT,\n",
    "               MPI_COMM_WORLD);\n",
    "  printf(\"[Collective] Rank %d received alltoall data: \", rank);\n",
    "  for (int i = 0; i < size; i++) {\n",
    "    printf(\"%d \", alltoall_recv[i]);\n",
    "  }\n",
    "  printf(\"\\n\");\n",
    "\n",
    "  // Cleanup\n",
    "  free(alltoall_send);\n",
    "  free(alltoall_recv);\n",
    "  if (rank == 0) {\n",
    "    free(scatter_sendbuf);\n",
    "    free(gather_recvbuf);\n",
    "  }\n",
    "\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c-collective-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collective] Rank 0 received bcast_data: 100\n",
      "[Collective] Rank 1 received bcast_data: 100\n",
      "[Collective] Rank 2 received bcast_data: 100\n",
      "[Collective] Rank 3 received bcast_data: 100\n",
      "[Collective] Rank 0 received scatter_recvbuf: 10\n",
      "[Collective] Rank 1 received scatter_recvbuf: 20\n",
      "[Collective] Rank 2 received scatter_recvbuf: 30\n",
      "[Collective] Rank 3 received scatter_recvbuf: 40\n",
      "[Collective] Rank 0 gathered values: 0 1 2 3 \n",
      "[Collective] Sum of ranks (reduce): 6\n",
      "[Collective] Rank 0 has allreduce sum: 6\n",
      "[Collective] Rank 1 has allreduce sum: 6\n",
      "[Collective] Rank 2 has allreduce sum: 6\n",
      "[Collective] Rank 3 has allreduce sum: 6\n",
      "[Collective] Rank 0 received alltoall data: 0 100 200 300 \n",
      "[Collective] Rank 1 received alltoall data: 1 101 201 301 \n",
      "[Collective] Rank 2 received alltoall data: 2 102 202 302 \n",
      "[Collective] Rank 3 received alltoall data: 3 103 203 303 \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mpicc ./codes/4_collective.c -o ./codes/4_collective\n",
    "mpiexec -n 4 ./codes/4_collective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-derived",
   "metadata": {},
   "source": [
    "## 5. Derived Datatypes\n",
    "\n",
    "Define a custom MPI datatype for a complex structure. This allows sending a structure with multiple fields in one call.\n",
    "\n",
    "`MPI_Type_create_struct` : Create a custom MPI datatype from a struct\n",
    "\n",
    "`MPI_Type_commit` : Commit the custom datatype\n",
    "\n",
    "`MPI_Type_free` : Free the custom datatype\n",
    "\n",
    "`MPI_Aint` : Address type for MPI\n",
    "\n",
    "Code example\n",
    "\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stddef.h>\n",
    "\n",
    "typedef struct {\n",
    "    int id;\n",
    "    double value;\n",
    "    int data[3];\n",
    "} Record;\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "    int rank, size;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    if (size < 2) {\n",
    "        if (rank == 0) fprintf(stderr, \"Requires at least 2 processes\\n\");\n",
    "        MPI_Finalize();\n",
    "        return 1;\n",
    "    }\n",
    "\n",
    "    MPI_Datatype record_type;\n",
    "    int blocklengths[3] = {1, 1, 3};\n",
    "    MPI_Aint offsets[3];\n",
    "    offsets[0] = offsetof(Record, id);\n",
    "    offsets[1] = offsetof(Record, value);\n",
    "    offsets[2] = offsetof(Record, data);\n",
    "    MPI_Datatype types[3] = {MPI_INT, MPI_DOUBLE, MPI_INT};\n",
    "    MPI_Type_create_struct(3, blocklengths, offsets, types, &record_type);\n",
    "    MPI_Type_commit(&record_type);\n",
    "\n",
    "    if (rank == 0) {\n",
    "        Record rec = {42, 3.14159, {7,8,9}};\n",
    "        MPI_Send(&rec, 1, record_type, 1, 0, MPI_COMM_WORLD);\n",
    "        printf(\"[Derived] Process 0 sent record: id=%d, value=%f\\n\", rec.id, rec.value);\n",
    "    } else if (rank == 1) {\n",
    "        Record rec;\n",
    "        MPI_Recv(&rec, 1, record_type, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
    "        printf(\"[Derived] Process 1 received record: id=%d, value=%f\\n\", rec.id, rec.value);\n",
    "    }\n",
    "    MPI_Type_free(&record_type);\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c-derived-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Derived] Process 0 sent record: id=42, value=3.141590\n",
      "[Derived] Process 1 received record: id=42, value=3.141590\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Compile and run\n",
    "mpicc ./codes/5_derived_datatype.c -o ./codes/5_derived_datatype\n",
    "mpiexec -n 2 ./codes/5_derived_datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-subcomm",
   "metadata": {},
   "source": [
    "## 6. Subcommunicators\n",
    "\n",
    "Split `MPI_COMM_WORLD` into subcommunicators. In this example, processes are divided based on even/odd ranks.\n",
    "\n",
    "`MPI_Comm_split` : Split a communicator into subgroups\n",
    "\n",
    "`MPI_Comm_free` : Free a communicator\n",
    "\n",
    "Code example\n",
    "\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "    int rank, size;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    int color = rank % 2;\n",
    "    MPI_Comm subcomm;\n",
    "    MPI_Comm_split(MPI_COMM_WORLD, color, rank, &subcomm);\n",
    "    int subrank, subsize;\n",
    "    MPI_Comm_rank(subcomm, &subrank);\n",
    "    MPI_Comm_size(subcomm, &subsize);\n",
    "    printf(\"[Subcomm] Global rank %d in group %d, subrank %d (subsize %d)\\n\", rank, color, subrank, subsize);\n",
    "    MPI_Comm_free(&subcomm);\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c-subcomm-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Subcomm] Global rank 0 in group 0, subrank 0 (subsize 2)\n",
      "[Subcomm] Global rank 2 in group 0, subrank 1 (subsize 2)\n",
      "[Subcomm] Global rank 1 in group 1, subrank 0 (subsize 2)\n",
      "[Subcomm] Global rank 3 in group 1, subrank 1 (subsize 2)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Compile and run\n",
    "mpicc ./codes/6_subcommunicators.c -o ./codes/6_subcommunicators\n",
    "mpiexec -n 4 ./codes/6_subcommunicators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-topology",
   "metadata": {},
   "source": [
    "## 7. Topology Mapping: Cartesian Grids\n",
    "\n",
    "Create a Cartesian communicator for applications with grid structure.\n",
    "\n",
    "`MPI_Dims_create` : Create dimensions for a Cartesian grid\n",
    "\n",
    "`MPI_Cart_create` : Create a Cartesian communicator\n",
    "\n",
    "`MPI_Cart_coords` : Get the coordinates of a process in a Cartesian grid\n",
    "\n",
    "Code example\n",
    "\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "    MPI_Init(&argc, &argv);\n",
    "    int rank, size;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "    int dims[2] = {0, 0};\n",
    "    MPI_Dims_create(size, 2, dims);\n",
    "    int periods[2] = {1, 1};\n",
    "    MPI_Comm cart_comm;\n",
    "    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 1, &cart_comm);\n",
    "    int cart_rank, coords[2];\n",
    "    MPI_Comm_rank(cart_comm, &cart_rank);\n",
    "    MPI_Cart_coords(cart_comm, cart_rank, 2, coords);\n",
    "    printf(\"[Cartesian] Global rank %d has coords: (%d, %d)\\n\", rank, coords[0], coords[1]);\n",
    "    MPI_Comm_free(&cart_comm);\n",
    "    MPI_Finalize();\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c-topology-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cartesian] Global rank 0 has coords: (0, 0)\n",
      "[Cartesian] Global rank 1 has coords: (0, 1)\n",
      "[Cartesian] Global rank 2 has coords: (1, 0)\n",
      "[Cartesian] Global rank 3 has coords: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Compile and run\n",
    "mpicc ./codes/7_cartesian_topology.c -o ./codes/7_cartesian_topology\n",
    "mpiexec -n 4 ./codes/7_cartesian_topology\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c-performance",
   "metadata": {},
   "source": [
    "## 8. Performance Measurement\n",
    "\n",
    "Use `MPI_Wtime()` to measure the time taken by sections of code. This is critical for tuning parallel applications.\n",
    "\n",
    "`MPI_Barrier` : Synchronize all processes\n",
    "\n",
    "`MPI_Wtime` : Get the current time in seconds\n",
    "\n",
    "Code example\n",
    "\n",
    "```c\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <unistd.h>\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "  MPI_Init(&argc, &argv);\n",
    "  MPI_Barrier(MPI_COMM_WORLD);\n",
    "  double start = MPI_Wtime();\n",
    "  MPI_Barrier(MPI_COMM_WORLD);\n",
    "  sleep(1);\n",
    "  double end = MPI_Wtime();\n",
    "  int rank;\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "  if (rank == 0) {\n",
    "    printf(\"[Performance] Barrier took %f seconds\\n\", end - start);\n",
    "  }\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c-performance-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Performance] Barrier took 1.005063 seconds\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Compile and run\n",
    "mpicc ./codes/8_performance_measure.c -o ./codes/8_performance_measure\n",
    "mpiexec -n 4 ./codes/8_performance_measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29860ed5",
   "metadata": {},
   "source": [
    "## 9. Simple example for calculating PI using MPI\n",
    "\n",
    "\n",
    "\n",
    "```c\n",
    "#include <math.h>\n",
    "#include <mpi.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "  int n;                      // total number of intervals\n",
    "  int i;                      // loop variable\n",
    "  int rank, size;             // MPI process rank and number of processes\n",
    "  double h;                   // width of each interval\n",
    "  double x;                   // sample point in the interval\n",
    "  double local_sum = 0.0, pi; // partial sum and final result\n",
    "\n",
    "  // Initialize MPI\n",
    "  MPI_Init(&argc, &argv);\n",
    "  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
    "  MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
    "\n",
    "  // Process 0 reads the command-line argument.\n",
    "  if (rank == 0) {\n",
    "    if (argc != 2) {\n",
    "      fprintf(stderr, \"Usage: %s <num_intervals>\\n\", argv[0]);\n",
    "      MPI_Abort(MPI_COMM_WORLD, 1);\n",
    "    }\n",
    "    n = atoll(argv[1]);\n",
    "  }\n",
    "  double t_start = MPI_Wtime();\n",
    "\n",
    "  // Broadcast the number of intervals to all processes\n",
    "  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
    "\n",
    "  // Compute the width of each interval\n",
    "  h = 1.0 / n;\n",
    "\n",
    "  /*\n",
    "   * Each process computes its portion of the sum.\n",
    "   * Process 'rank' handles the intervals:\n",
    "   *   i = rank, rank + size, rank + 2*size, ...\n",
    "   *\n",
    "   * The approximation to the integral for each interval is:\n",
    "   *   (1/n) * (4 / (1 + x*x))   where x = i/n.\n",
    "   */\n",
    "  for (i = rank; i < n; i += size) {\n",
    "    x = i * h;\n",
    "    local_sum += 4.0 / (1.0 + x * x);\n",
    "  }\n",
    "  local_sum = local_sum * h;\n",
    "\n",
    "  // Use MPI_Reduce to sum up all the partial results into pi (only on rank 0)\n",
    "  MPI_Reduce(&local_sum, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n",
    "\n",
    "  double t_end = MPI_Wtime();\n",
    "  if (rank == 0) {\n",
    "    printf(\"Calculated Pi = %.16f\\n\", pi);\n",
    "    printf(\"Computation Time: %.10f seconds\\n\", t_end - t_start);\n",
    "    double expected_pi = 3.141592653589793;\n",
    "    double error = fabs(pi - expected_pi);\n",
    "    printf(\"Error: %.8e\\n\", error);\n",
    "  }\n",
    "\n",
    "  MPI_Finalize();\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a597142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Pi = 3.1415926545897164\n",
      "Computation Time: 0.3780540000 seconds\n",
      "Error: 9.99923255e-10\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Compile and run\n",
    "# Usage ./codes/9_final <number of elements>\n",
    "\n",
    "mpicc ./codes/9_final.c -o ./codes/9_final\n",
    "mpiexec -n 4 ./codes/9_final 1000000000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
