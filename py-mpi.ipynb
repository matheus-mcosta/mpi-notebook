{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "setup-inst",
   "metadata": {},
   "source": [
    "# Advanced MPI Programming with mpi4py\n",
    "\n",
    "This notebook is designed for advanced users and researchers. We cover the following topics:\n",
    "\n",
    "1. Basic MPI Environment: Initialization, Rank, Size\n",
    "2. Point-to-Point Communication: Blocking Send/Recv\n",
    "3. Non-blocking Communication: Isend/Irecv with Wait/Waitall\n",
    "4. Collective Communication: Broadcast, Scatter, Gather, Reduce, Allreduce, Alltoall\n",
    "5. Derived Datatypes and NumPy Integration\n",
    "6. Subcommunicators and Communicator Splitting\n",
    "7. Topology Mapping: Cartesian Grids\n",
    "8. Performance Measurement: MPI_Wtime\n",
    "9. Final example: Calculating PI\n",
    "\n",
    "Make sure you have an MPI installation (OpenMPI/MPICH) and `mpi4py` installed:\n",
    "```bash\n",
    "pip install mpi4py\n",
    "```\n",
    "\n",
    "To run the examples using 4 processes, for instance:\n",
    "```bash\n",
    "mpiexec -n 4 python example_file.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "topic-2-basic",
   "metadata": {},
   "source": [
    "## 1. Basic MPI Environment\n",
    "\n",
    "Initialize MPI, determine each process's rank and the total number of processes. This is the foundation of all MPI programs.\n",
    "\n",
    "`comm = MPI.COMM_WORLD` is the default communicator that includes all processes. `rank` is the process ID, and `size` is the total number of processes.\n",
    "\n",
    "`comm.Get_rank()`: Get the rank of the current process.\n",
    "\n",
    "`comm.Get_size()`: Get the total number of processes.\n",
    "\n",
    "\n",
    "Code example:\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "print(f\"[Basic] Process {rank} of {size}\")\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "basic-env-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Basic] Process 0 of 4\n",
      "[Basic] Process 1 of 4\n",
      "[Basic] Process 2 of 4\n",
      "[Basic] Process 3 of 4\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run script\n",
    "mpirun -n 4 python3 ./py-codes/1_hello_basic.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "topic-3-p2p",
   "metadata": {},
   "source": [
    "## 2. Point-to-Point Communication\n",
    "\n",
    "Demonstrate blocking communication with `Send` and `Recv`. In this example, process 0 sends a message to process 1.\n",
    "\n",
    "`comm.Send([data, datatype], dest=1, tag=0)`: Send data to a process.\n",
    "\n",
    "`comm.Recv([data, datatype], source=0, tag=0)`: Receive data from a process.\n",
    "\n",
    "Code example:\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if size < 2:\n",
    "    raise Exception(\"This example requires at least 2 processes.\")\n",
    "\n",
    "if rank == 0:\n",
    "    data = np.array([1, 2, 3], dtype=np.int32)\n",
    "    comm.Send([data, MPI.INT], dest=1, tag=0)\n",
    "    print(f\"[P2P] Process {rank} sent {data} to process 1\")\n",
    "elif rank == 1:\n",
    "    data = np.empty(3, dtype=np.int32)\n",
    "    comm.Recv([data, MPI.INT], source=0, tag=0)\n",
    "    print(f\"[P2P] Process {rank} received {data} from process 0\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2p-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0 sent 1 to process 1\n",
      "Process 1 received [1 2 3] from process 0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run script\n",
    "mpirun -n 2 python3 ./py-codes/2_blocking_comm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "topic-4-nonblocking",
   "metadata": {},
   "source": [
    "## 3. Non-blocking Communication\n",
    "\n",
    "Use `Isend` and `Irecv` for nonblocking communications. Here we demonstrate how a process can initiate a send or receive and continue with computation.\n",
    "\n",
    "```comm.Isend([data, datatype], dest=1, tag=0)```: Initiate a non-blocking send.\n",
    "\n",
    "```comm.Irecv([data, datatype], source=0, tag=0)```: Initiate a non-blocking receive.\n",
    "\n",
    "```req.Wait()```: Wait for the non-blocking operation to complete.\n",
    "\n",
    "Code example:\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if size < 2:\n",
    "    raise Exception(\"This example requires at least 2 processes.\")\n",
    "\n",
    "if rank == 0:\n",
    "    data = np.array([123], dtype=np.int32)\n",
    "    # Simulate computation\n",
    "    print(f\"[Nonblocking] Process {rank} is sending {data[0]}\")\n",
    "    req = comm.Isend([data, MPI.INT], dest=1, tag=0)\n",
    "    # Simulate computation\n",
    "    print(f\"[Nonblocking] Process {rank} is doing some computation\")\n",
    "    time.sleep(4)\n",
    "    req.Wait()  # Ensure the send has completed\n",
    "    print(f\"[Nonblocking] Process {rank} completed send of {data[0]} after req.Wait\")\n",
    "elif rank == 1:\n",
    "    data = np.empty(1, dtype=np.int32)\n",
    "    req = comm.Irecv([data, MPI.INT], source=0, tag=0)\n",
    "    # Simulate computation\n",
    "    print(f\"[Nonblocking] Process {rank} is doing some computation\")\n",
    "    time.sleep(2)\n",
    "    req.Wait()  # Ensure the receive has completed\n",
    "    print(f\"[Nonblocking] Process {rank} completed receive with {data[0]} after req.Wait\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "nonblocking-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Nonblocking] Process 0 is sending 123\n",
      "[Nonblocking] Process 0 is doing some computation\n",
      "[Nonblocking] Process 1 is doing some computation\n",
      "[Nonblocking] Process 1 completed receive with 123 after req.Wait\n",
      "[Nonblocking] Process 0 completed send of 123 after req.Wait\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run script\n",
    "mpirun -n 2 python3 ./py-codes/3_non_blocking_comm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "topic-5-collective",
   "metadata": {},
   "source": [
    "## 4. Collective Communication\n",
    "\n",
    "Collective operations involve all processes in a communicator. We cover broadcast, scatter, gather, reduce, allreduce, and all-to-all.\n",
    "\n",
    "`comm.bcast(data, root=0)`: Broadcast data from one process to all others.\n",
    "\n",
    "`comm.scatter(sendbuf, recvbuf, root=0)`: Scatter an array from one process to all others.\n",
    "\n",
    "`comm.gather(sendbuf, recvbuf, root=0)`: Gather an array from all processes to one.\n",
    "\n",
    "`comm.reduce(sendbuf, recvbuf, op=MPI.SUM, root=0)`: Reduce data from all processes to one.\n",
    "\n",
    "`comm.allreduce(sendbuf, recvbuf, op=MPI.SUM)`: Reduce data from all processes and distribute the result to all.\n",
    "\n",
    "`comm.alltoall(sendbuf, recvbuf)`: Exchange data between all processes.\n",
    "\n",
    "Code example:\n",
    "```python\n",
    "# Import mpi4py's MPI module\n",
    "import time\n",
    "from mpi4py import MPI\n",
    "\n",
    "# Set up MPI communicator and get parameters\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# MPI_Bcast: Broadcast from process 0 to all processes\n",
    "if rank == 0:\n",
    "    bcast_data = 100\n",
    "else:\n",
    "    bcast_data = None\n",
    "bcast_data = comm.bcast(bcast_data, root=0)\n",
    "print(f\"[Collective] Rank {rank} received bcast_data: {bcast_data}\")\n",
    "time.sleep(1)\n",
    "\n",
    "# MPI_Scatter: Scatter an array from process 0 to all processes\n",
    "if rank == 0:\n",
    "    scatter_data = [i * 10 for i in range(size)]\n",
    "else:\n",
    "    scatter_data = None\n",
    "scatter_recv = comm.scatter(scatter_data, root=0)\n",
    "print(f\"[Collective] Rank {rank} received scatter data: {scatter_recv}\")\n",
    "time.sleep(1)\n",
    "\n",
    "# MPI_Gather: Gather data from all processes to process 0\n",
    "send_val = rank\n",
    "gather_data = comm.gather(send_val, root=0)\n",
    "if rank == 0:\n",
    "    print(\"[Collective] Gathered data:\", gather_data)\n",
    "time.sleep(1)\n",
    "\n",
    "# MPI_Reduce: Reduce data from all processes (sum) and send result to process 0\n",
    "reduce_sum = comm.reduce(send_val, op=MPI.SUM, root=0)\n",
    "if rank == 0:\n",
    "    print(\"[Collective] Sum of ranks (reduce):\", reduce_sum)\n",
    "time.sleep(1)\n",
    "\n",
    "# MPI_Allreduce: Reduce data (sum) and distribute the result to all processes\n",
    "allreduce_sum = comm.allreduce(send_val, op=MPI.SUM)\n",
    "print(f\"[Collective] Rank {rank} has allreduce sum: {allreduce_sum}\")\n",
    "time.sleep(1)\n",
    "\n",
    "# MPI_Alltoall: Exchange data between all processes\n",
    "alltoall_send = [rank * 100 + i for i in range(size)]\n",
    "alltoall_recv = comm.alltoall(alltoall_send)\n",
    "print(f\"[Collective] Rank {rank} received alltoall data: {alltoall_recv}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "collective-bcast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Collective] Rank 0 received bcast_data: 100\n",
      "[Collective] Rank 2 received bcast_data: 100\n",
      "[Collective] Rank 1 received bcast_data: 100\n",
      "[Collective] Rank 3 received bcast_data: 100\n",
      "[Collective] Rank 0 received scatter data: 0\n",
      "[Collective] Rank 2 received scatter data: 20\n",
      "[Collective] Rank 1 received scatter data: 10\n",
      "[Collective] Rank 3 received scatter data: 30\n",
      "[Collective] Gathered data: [0, 1, 2, 3]\n",
      "[Collective] Sum of ranks (reduce): 6\n",
      "[Collective] Rank 0 has allreduce sum: 6\n",
      "[Collective] Rank 1 has allreduce sum: 6\n",
      "[Collective] Rank 2 has allreduce sum: 6\n",
      "[Collective] Rank 3 has allreduce sum: 6\n",
      "[Collective] Rank 0 received alltoall data: [0, 100, 200, 300]\n",
      "[Collective] Rank 1 received alltoall data: [1, 101, 201, 301]\n",
      "[Collective] Rank 2 received alltoall data: [2, 102, 202, 302]\n",
      "[Collective] Rank 3 received alltoall data: [3, 103, 203, 303]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run script\n",
    "mpirun -n 4 python3 ./py-codes/4_collective.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "topic-6-derived",
   "metadata": {},
   "source": [
    "## 5. Derived Datatypes and NumPy Integration\n",
    "\n",
    "While Python is dynamically typed, using NumPy arrays with MPI can be considered analogous to using derived datatypes in C. In mpi4py, NumPy arrays can be sent directly.\n",
    "\n",
    "`MPI.Datatype.Create_struct(blocklengths, offsets, types)`: Create a derived datatype.\n",
    "\n",
    "`MPI.Datatype.Create_resized(lb, extent)`: Resize a datatype to match the size of a NumPy data type.\n",
    "\n",
    "`datatype.Commit()`: Commit the datatype for use.\n",
    "\n",
    "`datatype.Free()`: Free the datatype after use.\n",
    "\n",
    "\n",
    "\n",
    "Code example:\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "# Define a structured NumPy data type.\n",
    "derived_dtype = np.dtype([\n",
    "    ('id', np.int32),\n",
    "    ('value',   np.float64),\n",
    "    ('timestamp', np.float64)\n",
    "])\n",
    "\n",
    "# Compute block lengths, displacements (offsets), and MPI types.\n",
    "blocklengths = [1, 1, 1]\n",
    "offsets = [derived_dtype.fields[field][1] for field in derived_dtype.names]\n",
    "types = [MPI.INT, MPI.DOUBLE, MPI.DOUBLE]\n",
    "\n",
    "# Create the MPI datatype\n",
    "derived_mpi_type = MPI.Datatype.Create_struct(blocklengths, offsets, types)\n",
    "derived_mpi_type.Commit()\n",
    "\n",
    "# Align it properly to match NumPy's structure size\n",
    "aligned_derived_mpi_type = derived_mpi_type.Create_resized(0, derived_dtype.itemsize)\n",
    "aligned_derived_mpi_type.Commit()\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if size < 2:\n",
    "    raise Exception(\"This example requires at least 2 processes.\")\n",
    "\n",
    "if rank == 0:\n",
    "    sensor_data = np.array([\n",
    "        (101, 23.45, 1618033.98),\n",
    "        (102, 27.89, 1618034.98)\n",
    "    ], dtype=derived_dtype)\n",
    "    \n",
    "    comm.Send([sensor_data, aligned_derived_mpi_type], dest=1, tag=42)\n",
    "    print(f\"[Derived] Process {rank} sent sensor data:\")\n",
    "    print(sensor_data)\n",
    "    \n",
    "elif rank == 1:\n",
    "    sensor_data_recv = np.empty(2, dtype=derived_dtype)\n",
    "    comm.Recv([sensor_data_recv, aligned_derived_mpi_type], source=0, tag=42)\n",
    "    print(f\"[Derived] Process {rank} received sensor data:\")\n",
    "    print(sensor_data_recv)\n",
    "\n",
    "aligned_derived_mpi_type.Free()\n",
    "derived_mpi_type.Free()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "derived-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Derived] Process 0 sent sensor data:\n",
      "[(101, 23.45, 1618033.98) (102, 27.89, 1618034.98)]\n",
      "[Derived] Process 1 received sensor data:\n",
      "[(101, 23.45, 1618033.98) (102, 27.89, 1618034.98)]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run script\n",
    "mpirun -n 2 python3 ./py-codes/5_derived.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "topic-7-subcomm",
   "metadata": {},
   "source": [
    "## 6. Subcommunicators and Communicator Splitting\n",
    "\n",
    "Splitting communicators allows you to create groups of processes that can work on subproblems. In this example, we split processes based on even/odd rank.\n",
    "\n",
    "`comm.Split(color, key)`: Split the communicator based on color and key.\n",
    "\n",
    "Code example:\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Each process selects a color based on even/odd rank\n",
    "color = rank % 2\n",
    "\n",
    "# Split the communicator into subcommunicators based on the color.\n",
    "subcomm = comm.Split(color, rank)\n",
    "\n",
    "subrank = subcomm.Get_rank()\n",
    "subsize = subcomm.Get_size()\n",
    "\n",
    "print(f\"[Subcomm] Global rank {rank} in group {color}, subrank {subrank} (subsize {subsize})\")\n",
    "\n",
    "subcomm.Free()\n",
    "MPI.Finalize()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "subcomm-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Subcomm] Global rank 0 in group 0, subrank 0 (subsize 2)\n",
      "[Subcomm] Global rank 1 in group 1, subrank 0 (subsize 2)\n",
      "[Subcomm] Global rank 2 in group 0, subrank 1 (subsize 2)\n",
      "[Subcomm] Global rank 3 in group 1, subrank 1 (subsize 2)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run script\n",
    "mpirun -n 4 python3 ./py-codes/6_subcommunicators.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "topic-8-topology",
   "metadata": {},
   "source": [
    "## 7. Topology Mapping: Cartesian Grids\n",
    "\n",
    "Cartesian communicators are useful for structured grid applications. Here we create a 2D grid communicator.\n",
    "\n",
    "`MPI.Compute_dims(size, ndims)`: Compute the dimensions of a Cartesian grid.\n",
    "\n",
    "`comm.Create_cart(dims, periods, reorder)`: Create a Cartesian communicator.\n",
    "\n",
    "`comm.Get_coords(rank)`: Get the coordinates of a process in the grid.\n",
    "\n",
    "\n",
    "Code example:\n",
    "```python\n",
    "# python\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# Create a 2D Cartesian grid communicator.\n",
    "dims = MPI.Compute_dims(size, 2)\n",
    "periods = [True, True]\n",
    "reorder = True\n",
    "cart_comm = comm.Create_cart(dims, periods=periods, reorder=reorder)\n",
    "\n",
    "cart_rank = cart_comm.Get_rank()\n",
    "coords = cart_comm.Get_coords(cart_rank)\n",
    "\n",
    "print(f\"[Cartesian] Global rank {rank} has coords: {coords}\")\n",
    "\n",
    "cart_comm.Free()\n",
    "MPI.Finalize()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cartesian-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cartesian] Global rank 0 has coords: [0, 0]\n",
      "[Cartesian] Global rank 1 has coords: [0, 1]\n",
      "[Cartesian] Global rank 2 has coords: [1, 0]\n",
      "[Cartesian] Global rank 3 has coords: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run script\n",
    "mpirun -n 4 python3 ./py-codes/7_cartesian.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "topic-11-performance",
   "metadata": {},
   "source": [
    "## 8. Performance Measurement\n",
    "\n",
    "Use `MPI.Wtime()` to measure elapsed time. In performance-critical applications, time barriers and collective operations to optimize communication.\n",
    "\n",
    "`MPI.Wtime()`: Get the current time in seconds.\n",
    "\n",
    "Code example:\n",
    "```python\n",
    "# python\n",
    "from mpi4py import MPI\n",
    "import time\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "# Synchronize processes.\n",
    "comm.Barrier()\n",
    "start = MPI.Wtime()\n",
    "\n",
    "comm.Barrier()\n",
    "time.sleep(1)  # Simulate work\n",
    "\n",
    "end = MPI.Wtime()\n",
    "\n",
    "if rank == 0:\n",
    "    print(f\"[Performance] Barrier took {end - start:.6f} seconds\")\n",
    "\n",
    "MPI.Finalize()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "performance-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Performance] Barrier took 1.000600 seconds\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run script\n",
    "mpirun -n 2 python3 ./py-codes/8_performance_measure.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "topic-12-final",
   "metadata": {},
   "source": [
    "## 9. Simple example for calculating PI using MPI\n",
    "\n",
    "\n",
    "```python\n",
    "from mpi4py import MPI\n",
    "import sys\n",
    "import math\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# Process 0 reads the number of intervals from the command line.\n",
    "if rank == 0:\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python3 9_final.py <number_of_intervals>\")\n",
    "        comm.Abort(1)\n",
    "    try:\n",
    "        n = int(sys.argv[1])\n",
    "    except ValueError:\n",
    "        print(\"Please provide an integer for number_of_intervals.\")\n",
    "        comm.Abort(2)\n",
    "else:\n",
    "    n = None\n",
    "\n",
    "# Broadcast the number of intervals to all processes\n",
    "n = comm.bcast(n, root=0)\n",
    "\n",
    "h = 1.0 / n\n",
    "local_sum = 0.0\n",
    "\n",
    "t_start = MPI.Wtime()\n",
    "\n",
    "# Each process computes its portion of the sum.\n",
    "# Process 'rank' handles i = rank, rank+size, rank+2*size, ...\n",
    "for i in range(rank, n, size):\n",
    "    x = i * h\n",
    "    local_sum += 4.0 / (1.0 + x * x)\n",
    "local_sum *= h\n",
    "\n",
    "# Reduce all partial sums into the final value of pi on process 0.\n",
    "pi = comm.reduce(local_sum, op=MPI.SUM, root=0)\n",
    "t_end = MPI.Wtime()\n",
    "\n",
    "if rank == 0:\n",
    "    error = abs(pi - math.pi)\n",
    "    print(f\"Calculated Pi = {pi:.16f}\")\n",
    "    print(f\"Elapsed time = {t_end - t_start:.6f} seconds\")\n",
    "    print(f\"Error = {error:.16f}\")\n",
    "\n",
    "MPI.Finalize()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "final-example-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Pi = 3.1415926635898743\n",
      "Elapsed time = 2.942581 seconds\n",
      "Error = 1.000008e-08\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Run script\n",
    "mpirun -n 4 python3 ./py-codes/9_final.py 100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-python",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have covered advanced MPI topics in Python using mpi4py including process management, collective communication, derived datatypes, subcommunicators, topology mapping, performance measurement, and a final real-life example of parallel matrix multiplication. Use these building blocks to develop scalable and efficient HPC applications.\n",
    "\n",
    "Happy HPC programming!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
